{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID PS and RX dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/manuel/Desktop/BiomedDataAnalysisCourse/project/data/\"\n",
    "# filled some missing values and corrected mistakes\n",
    "ps_rx_fname = os.path.join(data_path, \"merged_data_processed_corrected.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions used throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_age(birth_year: int, visit_year: int) -> int:\n",
    "    assert isinstance(birth_year, int)\n",
    "    assert isinstance(visit_year, int)\n",
    "    assert birth_year < visit_year\n",
    "    age = visit_year - birth_year\n",
    "    return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_column(column_data: pd.Series, dtype: str) -> List[Any]:\n",
    "    assert isinstance(column_data, pd.Series)\n",
    "    assert isinstance(dtype, str)\n",
    "    if dtype == \"categorical\":\n",
    "        most_freq = column_data.describe()[\"top\"]\n",
    "        column_data.fillna(most_freq, inplace=True)\n",
    "    elif dtype == \"numerical\":\n",
    "        mean_val = column_data.describe()[\"mean\"]\n",
    "        column_data.fillna(mean_val, inplace=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data type ({dtype})\")\n",
    "    return column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(X: pd.DataFrame, n_components: Optional[int]=2) -> pd.DataFrame:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(X)\n",
    "    X_pcs = pd.DataFrame(\n",
    "        data=pcs, \n",
    "        columns=[f\"PC{i}\" for i in range(1, (n_components + 1))],\n",
    "        index=X.index.tolist()\n",
    "    )\n",
    "    assert X.shape[0] == X_pcs.shape[0]\n",
    "    return X_pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(\n",
    "    X_pcs: pd.DataFrame, \n",
    "    title: str,\n",
    "    pc1: Optional[str]=\"PC1\",\n",
    "    pc2: Optional[str]=\"PC2\"\n",
    ") -> None:\n",
    "    assert isinstance(X_pcs, pd.DataFrame)\n",
    "    assert isinstance(title, str)\n",
    "    assert isinstance(pc1, str)\n",
    "    assert pc1 in X_pcs.columns.tolist()\n",
    "    assert isinstance(pc2, str)\n",
    "    assert pc2 in X_pcs.columns.tolist()\n",
    "    f,ax = plt.subplots(1,1,figsize=(15,10))\n",
    "    sns.scatterplot(data=X_pcs, x=pc1, y=pc2, ax=ax)\n",
    "    ax.set_xlabel(pc1, size=16)\n",
    "    ax.set_ylabel(pc2, size=16)\n",
    "    ax.set_title(title, size=18)\n",
    "    plt.show()  # display plot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_measures(column_data: pd.Series) -> List[float]:\n",
    "    assert isinstance(column_data, pd.Series)\n",
    "    corrected_data = []\n",
    "    for v in column_data.tolist():\n",
    "        if str(v) == \"nan\":\n",
    "            corrected_data.append(v)\n",
    "        else:\n",
    "            fields = v.split(\"10^9\")\n",
    "            value = float(fields[0].replace(\",\", \".\"))\n",
    "            corrected_data.append(value)\n",
    "    assert len(column_data) == len(corrected_data)\n",
    "    return corrected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(X, Y, predictions, title, f1_score):\n",
    "    assert isinstance(predictions, np.ndarray)\n",
    "    assert isinstance(title, str)\n",
    "    assert isinstance(f1_score, float)\n",
    "    pca = PCA(n_components=2)\n",
    "    pcs = pca.fit_transform(X)\n",
    "    X_pcs = pd.DataFrame(\n",
    "        data=pcs, columns=[f\"PC{i}\" for i in range(1,3)], index=Y.index.tolist()\n",
    "    )\n",
    "    X_pcs[\"Prediction\"] = predictions\n",
    "    X_pcs[\"Death\"] = Y.tolist()\n",
    "    f, (ax1, ax2) = plt.subplots(1,2,figsize=(20,10))\n",
    "    palette=[\"#D3880E\", \"#3B1375\"]\n",
    "    sns.scatterplot(data=X_pcs, x=\"PC1\", y=\"PC2\", palette=palette, hue=\"Death\", ax=ax1)\n",
    "    ax1.set_xlabel(\"PC1\", size=16)\n",
    "    ax1.set_ylabel(\"PC2\", size=16)\n",
    "    ax1.set_title(\"Original data\", size=18)\n",
    "    sns.scatterplot(data=X_pcs, x=\"PC1\", y=\"PC2\", palette=palette, hue=\"Prediction\", ax=ax2)\n",
    "    ax2.set_xlabel(\"PC1\", size=16)\n",
    "    ax2.set_ylabel(\"PC2\", size=16)\n",
    "    ax2.set_title(\" \".join([title, \"(F1-score: %.2f)\" % (f1_score)]), size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rx_df = pd.read_csv(ps_rx_fname, delimiter=\";\", decimal=\",\")\n",
    "ps_rx_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove units of measure from some columns of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two cols with units of measure\n",
    "for col in [\n",
    "    \"FIELDSET_PS-BLOOD_COUNT_LEUCOCITI\", \"FIELDSET_PS-BLOOD_COUNT_NEUTROFILI\"\n",
    "]:\n",
    "    ps_rx_df[col] = remove_measures(ps_rx_df[col])\n",
    "ps_rx_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute patients age at ER visit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rx_df[\"AGE\"] = ps_rx_df.apply(lambda x : compute_age(int(x[1]), int(x[-1].split(\"/\")[-1])), axis=1)\n",
    "ps_rx_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns not used throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"BIRTHDAY\",\n",
    "    \"DEAD_DATE\",\n",
    "    \"STOP\",\n",
    "    \"START\",\n",
    "    \"CODE\"\n",
    "]\n",
    "ps_rx_df.drop(drop_cols, axis=1, inplace=True)\n",
    "ps_rx_df.head()  # 769 visits and 89 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set visit ID as DataFrame index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rx_df.index = ps_rx_df.ID.tolist()\n",
    "ps_rx_df.drop([\"ID\"], axis=1, inplace=True)\n",
    "ps_rx_df.head()  # 88 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover training data and the response we want predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ps_rx_df.drop([\"DEATH\"], axis=1)\n",
    "Y = ps_rx_df[\"DEATH\"]\n",
    "X.head()  # training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()  # response -> death or survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical variables in the dataset. For encoding we'll use `OrdinalEncoder` function from the `sklearn` Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN rows for each categorical variable using the most frequent value\n",
    "cat_vars = X.select_dtypes([\"object\"]).columns.tolist()\n",
    "for cat_var in cat_vars:\n",
    "    X[cat_var] = fillna_column(X[cat_var], \"categorical\")\n",
    "# categorical variables encoding\n",
    "X_cat = X[cat_vars]\n",
    "enc = OrdinalEncoder()\n",
    "X[cat_vars] = enc.fit_transform(X_cat)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now count how many `NaN` values there are in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum().sum()  # 7452"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7452 `NaN` values in our data. They could create some problems when fitting models to our data:\n",
    "- can change some metrics, like mean, variance, median, etc.\n",
    "- `sklearn` models do not manage the presence of `NaN` values\n",
    "\n",
    "The easiest solution is to remove rows containing `NaN` values. However, we would remove too much observations from our datasets.<br>\n",
    "\n",
    "An alternative solution is to impute `NaN` values. Let's try imputing our values using KNN. KNN finds the closest `k` samples to the considered sample in our dataset, and impute the `NaN` value computing the mean value of the closest `k` samples. <br>\n",
    "\n",
    "Luckily, `sklearn` provides an implementation of such method. Let's use it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaN values using KNN \n",
    "imputer = KNNImputer(n_neighbors=5)  # use k == 5\n",
    "X = pd.DataFrame(data=imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that now `NaN` values have been replaced by the imputed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore our dataset to look for potential correlations among variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1,figsize=(15,15))\n",
    "sns.heatmap(X.corr(), linewidths=0.5, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are few variables correlating between each others. However, as expected blood values strongly correlates between each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 87 variables, so it is not possible to visualize how data distribute in a human interpretable manner. <br>\n",
    "\n",
    "Luckily, there exist dimensionality reduction techniques to visualize such multi-dimensional data. Let's use the **Principal Component Analysis (PCA)**. Briefly, PCA computes the components providing the most difference between dataset's samples. Moreover, it allows to visualize multidimensional datasets in two dimensions. <br>\n",
    "\n",
    "Let's now visualize our dataset using dimensionality reduction via PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pcs = compute_pca(X)\n",
    "plot_pca(X_pcs, \"Original dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there are some strong outliers in our dataset. We should remove them before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "visits_to_remove = X_pcs[(X_pcs.PC1 > 3500) | (X_pcs.PC2 > 1000)].index.tolist()\n",
    "X.drop(visits_to_remove, axis=0, inplace=True)\n",
    "# adjust Y accordingly\n",
    "Y.drop(visits_to_remove, axis=0, inplace=True)\n",
    "# recompute PCs\n",
    "X_pcs = compute_pca(X)\n",
    "# plot PCs\n",
    "plot_pca(X_pcs, \"Filtered dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data appear to be more distributed. However, they still seem to be very close to each other, and no clear separation can be observed. <br>\n",
    "Let's try to process our data through **normalization** and **standard scaling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with data standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize numerical data\n",
    "X_std = X.copy()\n",
    "numerical_columns = list(set(X_std.columns).difference(cat_vars))  # get numerical cols\n",
    "X_num = X_std[numerical_columns]\n",
    "scaler = StandardScaler()  # initialize scaler\n",
    "scaler.fit(X_num)  # scale data\n",
    "X_num = scaler.transform(X_num)\n",
    "X_std[numerical_columns] = X_num\n",
    "X_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot our data after standard scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PCs\n",
    "X_pcs = compute_pca(X_std)\n",
    "plot_pca(X_pcs, \"Standard scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly our dataset appears to be more distributed than before. Usually this is good, but let's see if there exists some separation between our two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class labels to PCs\n",
    "X_pcs[\"Death\"] = Y.tolist()\n",
    "# plot data\n",
    "palette=[\"#D3880E\", \"#3B1375\"]\n",
    "f,ax = plt.subplots(1,1,figsize=(15,10))\n",
    "sns.scatterplot(data=X_pcs, x=\"PC1\", y=\"PC2\", ax=ax, hue=\"Death\", palette=palette)\n",
    "ax.set_xlabel(\"PC1\", size=16)\n",
    "ax.set_ylabel(\"PC2\", size=16)\n",
    "ax.set_title(\"Standard scaling with labels\", size=18)\n",
    "plt.show()  # display plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was clear at first glance, there is no clear separation between our two classes. Let's see if with normalization something changes (very unlikely!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numerical data\n",
    "X_norm = X.copy()\n",
    "numerical_columns = list(set(X_norm.columns).difference(cat_vars))  # get numerical cols\n",
    "X_num = X_norm[numerical_columns]\n",
    "X_norm[numerical_columns] = normalize(X_num)  # normalize data\n",
    "X_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PCs\n",
    "X_pcs = compute_pca(X_norm)\n",
    "plot_pca(X_pcs, \"Normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is similar to standard scaling, so we don't expect that there will be a clear separation between our two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class labels to PCs\n",
    "X_pcs[\"Death\"] = Y.tolist()\n",
    "# plot data\n",
    "palette=[\"#D3880E\", \"#3B1375\"]\n",
    "f,ax = plt.subplots(1,1,figsize=(15,10))\n",
    "sns.scatterplot(data=X_pcs, x=\"PC1\", y=\"PC2\", ax=ax, hue=\"Death\", palette=palette)\n",
    "ax.set_xlabel(\"PC1\", size=16)\n",
    "ax.set_ylabel(\"PC2\", size=16)\n",
    "ax.set_title(\"Standard scaling with labels\", size=18)\n",
    "plt.show()  # display plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we completed our exploratory analysis, we can now begin trying to fit some models to our data and see what happens.<br>\n",
    "\n",
    "We will consider different ML models and methods suited for data classification:\n",
    "- **K-means**\n",
    "- **KNN**\n",
    "- **Logistic regression**\n",
    "- **SVM (linear kernel)**\n",
    "- **Decision trees**\n",
    "\n",
    "In the following we will use original, standardized, and normalized data.\n",
    "\n",
    "Each model will be evaluated using:\n",
    "- **Mean accuracy**\n",
    "- **F1-score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute clusters with k-means\n",
    "kmeans = KMeans(\n",
    "    n_clusters=2, init=\"random\", max_iter=1000, random_state=0\n",
    ")\n",
    "# run 10 times\n",
    "f1 = []\n",
    "for _ in range(10): \n",
    "    predictions = kmeans.fit_predict(X)\n",
    "    # compute F1-score\n",
    "    f1.append(f1_score(Y, predictions, average=\"binary\"))\n",
    "assert len(f1) == 10\n",
    "# visualize results\n",
    "visualize_results(X, Y, predictions, \"K-means\", np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute clusters with k-means\n",
    "kmeans = KMeans(\n",
    "    n_clusters=2, init=\"random\", max_iter=1000, random_state=0\n",
    ")\n",
    "# run 10 times\n",
    "f1 = []\n",
    "for _ in range(10): \n",
    "    predictions = kmeans.fit_predict(X_std)\n",
    "    # compute F1-score\n",
    "    f1.append(f1_score(Y, predictions, average=\"binary\"))\n",
    "assert len(f1) == 10\n",
    "# visualize results\n",
    "visualize_results(X_std, Y, predictions, \"K-means\", np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute clusters with k-means\n",
    "kmeans = KMeans(\n",
    "    n_clusters=2, init=\"random\", max_iter=1000, random_state=0\n",
    ")\n",
    "# run 10 times\n",
    "f1 = []\n",
    "for _ in range(10): \n",
    "    predictions = kmeans.fit_predict(X_norm)\n",
    "    # compute F1-score\n",
    "    f1.append(f1_score(Y, predictions, average=\"binary\"))\n",
    "assert len(f1) == 10\n",
    "# visualize results\n",
    "visualize_results(X_norm, Y, predictions, \"K-means\", np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute clusters with KNN\n",
    "scores_tot = []\n",
    "for n in range(3, 11):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "    scores = cross_val_score(\n",
    "        neigh, X, Y, cv=6, scoring=\"f1_macro\"\n",
    "    )\n",
    "    scores_tot.append(np.mean(scores))\n",
    "best = -1\n",
    "best_idx = -1\n",
    "for i,s in enumerate(scores_tot):\n",
    "    if s > best:\n",
    "        best = s\n",
    "        best_idx = i\n",
    "assert best > 0\n",
    "assert best_idx >= 0\n",
    "print(f\"Best F1-score obtained using {best_idx + 3} neighbours\")\n",
    "print(\"Mean F1-score: %.2f\" % (best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute clusters with KNN\n",
    "scores_tot = []\n",
    "for n in range(3, 11):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "    scores = cross_val_score(\n",
    "        neigh, X_std, Y, cv=6, scoring=\"f1_macro\"\n",
    "    )\n",
    "    scores_tot.append(np.mean(scores))\n",
    "best = -1\n",
    "best_idx = -1\n",
    "for i,s in enumerate(scores_tot):\n",
    "    if s > best:\n",
    "        best = s\n",
    "        best_idx = i\n",
    "assert best > 0\n",
    "assert best_idx >= 0\n",
    "print(f\"Best F1-score obtained using {best_idx + 3} neighbours\")\n",
    "print(\"Mean F1-score: %.2f\" % (best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute clusters with KNN\n",
    "scores_tot = []\n",
    "for n in range(3, 11):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "    scores = cross_val_score(\n",
    "        neigh, X_norm, Y, cv=6, scoring=\"f1_macro\"\n",
    "    )\n",
    "    scores_tot.append(np.mean(scores))\n",
    "best = -1\n",
    "best_idx = -1\n",
    "for i,s in enumerate(scores_tot):\n",
    "    if s > best:\n",
    "        best = s\n",
    "        best_idx = i\n",
    "assert best > 0\n",
    "assert best_idx >= 0\n",
    "print(f\"Best F1-score obtained using {best_idx + 3} neighbours\")\n",
    "print(\"Mean F1-score: %.2f\" % (best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data using logistic regression\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "acc = clf.score(X_test, Y_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"Accuracy: %.2f\" % (acc))\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize results\n",
    "visualize_results(X_test, Y_test, predictions, \"Original data\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross validation\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "scores = cross_val_score(\n",
    "    clf, X, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data using logistic regression\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_std, Y, test_size=0.25\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "acc = clf.score(X_test, Y_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"Accuracy: %.2f\" % (acc))\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize results\n",
    "visualize_results(X_test, Y_test, predictions, \"Original data\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross validation\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "scores = cross_val_score(\n",
    "    clf, X_std, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data using logistic regression\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=0.25\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "acc = clf.score(X_test, Y_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"Accuracy: %.2f\" % (acc))\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize results\n",
    "visualize_results(X_test, Y_test, predictions, \"Original data\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross validation\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "scores = cross_val_score(\n",
    "    clf, X_norm, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of our model are not bad, but neither good enough. There could be some variables introducing some bias within the model. Therefore, we should remove such variables, and train the model only on those features really characterizing the model.<br>\n",
    "\n",
    "To perform this task we will use **LASSO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try feature selection using LASSO\n",
    "clf = LogisticRegression(C=0.01, penalty=\"l2\", dual=False, max_iter=10000).fit(\n",
    "    X, Y\n",
    ")\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_feat_sel = model.transform(X)\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X.shape[0]} x {X.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_feat_sel.shape[0]} x {X_feat_sel.shape[1]}\"\n",
    ")  # kept 27 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model with cross validation\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "scores = cross_val_score(\n",
    "    clf, X_feat_sel, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we got better performances than before. Let's repeat the procedure on the standardized and normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try feature selection using LASSO\n",
    "clf = LogisticRegression(C=0.01, penalty=\"l2\", dual=False, max_iter=10000).fit(\n",
    "    X_std, Y\n",
    ")\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_feat_sel = model.transform(X_std)\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_std.shape[0]} x {X_std.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_feat_sel.shape[0]} x {X_feat_sel.shape[1]}\"\n",
    ")  # kept 34 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model with cross validation\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "scores = cross_val_score(\n",
    "    clf, X_feat_sel, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try feature selection using LASSO\n",
    "clf = LogisticRegression(C=0.01, penalty=\"l2\", dual=False, max_iter=10000).fit(\n",
    "    X_norm, Y\n",
    ")\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_feat_sel = model.transform(X_norm)\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_norm.shape[0]} x {X_norm.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_feat_sel.shape[0]} x {X_feat_sel.shape[1]}\"\n",
    ")  # kept 19 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model with cross validation\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "scores = cross_val_score(\n",
    "    clf, X_feat_sel, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data using SVM \n",
    "clf = LinearSVC(random_state=0, max_iter=100000)\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "acc = clf.score(X_test, Y_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"Accuracy: %.2f\" % (acc))\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize results\n",
    "visualize_results(X_test, Y_test, predictions, \"Original data\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model with cross validation\n",
    "clf = LinearSVC(random_state=0, max_iter=100000)\n",
    "scores = cross_val_score(\n",
    "    clf, X, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data using SVM \n",
    "clf = LinearSVC(random_state=0, max_iter=100000)\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_std, Y, test_size=0.25\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "acc = clf.score(X_test, Y_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"Accuracy: %.2f\" % (acc))\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize results\n",
    "visualize_results(X_test, Y_test, predictions, \"Original data\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model with cross validation\n",
    "clf = LinearSVC(random_state=0, max_iter=100000)\n",
    "scores = cross_val_score(\n",
    "    clf, X_std, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data using SVM \n",
    "clf = LinearSVC(random_state=0, max_iter=100000)\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=0.25\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "acc = clf.score(X_test, Y_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"Accuracy: %.2f\" % (acc))\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize results\n",
    "visualize_results(X_test, Y_test, predictions, \"Original data\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model with cross validation\n",
    "clf = LinearSVC(random_state=0, max_iter=100000)\n",
    "scores = cross_val_score(\n",
    "    clf, X_norm, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously done with logistic regression, let's try to perform some feature selection using LASSO to improve the performance of our SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try feature selection using LASSO\n",
    "clf = LinearSVC(C=0.01, penalty=\"l1\", dual=False, max_iter=100000).fit(X, Y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_feat_sel = model.transform(X)\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X.shape[0]} x {X.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_feat_sel.shape[0]} x {X_feat_sel.shape[1]}\"\n",
    ")  # kept 29 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross-validation\n",
    "clf = LinearSVC(max_iter=100000, random_state=0)\n",
    "scores = cross_val_score(\n",
    "    clf, X_feat_sel, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try feature selection using LASSO\n",
    "clf = LinearSVC(C=0.01, penalty=\"l1\", dual=False, max_iter=100000).fit(X_std, Y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_feat_sel = model.transform(X_std)\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_std.shape[0]} x {X_std.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_feat_sel.shape[0]} x {X_feat_sel.shape[1]}\"\n",
    ")  # kept 21 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross-validation\n",
    "clf = LinearSVC(max_iter=100000, random_state=0)\n",
    "scores = cross_val_score(\n",
    "    clf, X_feat_sel, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try feature selection using LASSO\n",
    "clf = LinearSVC(C=0.01, penalty=\"l1\", dual=False, max_iter=100000).fit(X_norm, Y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_feat_sel = model.transform(X_norm)\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_norm.shape[0]} x {X_norm.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Dataset shape before feature selection: {X_feat_sel.shape[0]} x {X_feat_sel.shape[1]}\"\n",
    ")  # kept 6 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross-validation\n",
    "clf = LinearSVC(max_iter=100000, random_state=0)\n",
    "scores = cross_val_score(\n",
    "    clf, X_feat_sel, Y, cv=6, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify our data using decision trees\n",
    "clf = DecisionTreeClassifier()\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25\n",
    ")\n",
    "# classify data\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize the tree\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross-validation\n",
    "clf = DecisionTreeClassifier()\n",
    "scores = cross_val_score(\n",
    "    clf, X, Y, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify our data using decision trees\n",
    "clf = DecisionTreeClassifier()\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_std, Y, test_size=0.25\n",
    ")\n",
    "# classify data\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize the tree\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross-validation\n",
    "clf = DecisionTreeClassifier()\n",
    "scores = cross_val_score(\n",
    "    clf, X_std, Y, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify our data using decision trees\n",
    "clf = DecisionTreeClassifier()\n",
    "# split dataset in training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=0.25\n",
    ")\n",
    "# classify data\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"binary\")\n",
    "print(\"F1-score: %.2f\" % (f1))\n",
    "# visualize the tree\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with cross-validation\n",
    "clf = DecisionTreeClassifier()\n",
    "scores = cross_val_score(\n",
    "    clf, X_norm, Y, scoring=\"f1_macro\"\n",
    ")\n",
    "print(\"Average F1-score: %.2f\" % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
